{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mar 15, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(num, 2)\n",
    "Y = np.zeros(num)\n",
    "Y[X[:, 0] * X[:, 0] > X[:, 1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(np.arange(num))\n",
    "train_x, train_y = X[train], Y[train]\n",
    "test_x, test_y = X[test], Y[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLayer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, X: np.ndarray) \\\n",
    "            -> np.ndarray:\n",
    "        raise Exception(\"Base class forward not implemented\")\n",
    "    def backward(self, grad: np.ndarray) \\\n",
    "            -> np.ndarray:\n",
    "        raise Exception(\"Base class backward not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(BaseLayer):\n",
    "    def __init__(self, head: int, size: int = 1):\n",
    "        self.head = head\n",
    "        self.params = np.random.rand(size, head)\n",
    "        self.bias = np.random.rand(size, 1)\n",
    "        \n",
    "        self.after, self.before = None, None\n",
    "        self.X, self.A = None, None\n",
    "        self.dW, self.db = 0, 0\n",
    "    \n",
    "    def forward(self, X: np.ndarray, grad: bool = True):\n",
    "        if grad:\n",
    "            if self.X is not None:\n",
    "                warnings.warn(\"Layer backward is not called after forward.\")\n",
    "            self.X = X\n",
    "            \n",
    "        self.A = sigmoid(np.dot(self.params, X) + self.bias)\n",
    "        \n",
    "        return self.after.forward(self.A) if self.after else self.A\n",
    "    \n",
    "    def backward(self, grad: np.ndarray):\n",
    "        def process(g):\n",
    "            return np.multiply(np.dot(self.after.params.T, g), 1 - np.power(self.A, 2))\n",
    "        \n",
    "        if self.X is None:\n",
    "            raise Exception(\"Layer backward call must after forward.\")\n",
    "            \n",
    "        dZ = process(self.after.backward(grad)) if self.after else (self.A - grad)\n",
    "        \n",
    "        self.dW += np.dot(dZ, self.X.T) / np.size(self.X.T, 0)\n",
    "        self.db += np.sum(dZ, axis=1, keepdims=True) / np.size(self.X.T, 0)\n",
    "        \n",
    "        # Release previous input\n",
    "        self.X = None\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "    def update(self, lr: float):\n",
    "        self.params = self.params - lr * self.dW\n",
    "        self.bias = self.bias - lr * self.db\n",
    "        \n",
    "        self.dW, self.db = 0, 0\n",
    "        \n",
    "        if self.after:\n",
    "            self.after.update(lr)\n",
    "        \n",
    "    def append(self, layer):\n",
    "        self.after = layer\n",
    "        layer.before = self\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu(Activation):\n",
    "    def forward(self, X):\n",
    "        return 1. / (1 + np.exp(-X))\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad * (1. - grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Layer(2, 4).append(\n",
    "    Layer(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:47<00:00, 42.05it/s, loss=0.122]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=epoch) as t:\n",
    "    for e in range(epoch):\n",
    "        forward = network.forward(train_x.T)\n",
    "        loss = -(train_y * np.log(forward) + (1 - train_y) * np.log(1 - forward))\n",
    "        backward = network.backward(train_y)\n",
    "        network.update(.5)\n",
    "        \n",
    "        t.set_postfix(loss=loss.mean())\n",
    "        t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95164"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.rint(network.forward(test_x.T, grad=False)) == test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network([\n",
    "    Layer(2, 4),\n",
    "    ReLU(),\n",
    "    Layer(4, 8),\n",
    "    ReLU(),\n",
    "    Layer(8, 4),\n",
    "    ReLU(),\n",
    "    Layer(4, 1),\n",
    "    ReLU(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(object):\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __init__(self, shape: tuple = (2,), lr: int = .1):\n",
    "        self.shape = shape\n",
    "        self.lr = lr\n",
    "        self.weights = np.random.rand(*shape)\n",
    "        self.bias = np.random.rand(1)\n",
    "    \n",
    "    def train(self, X: np.ndarray, Y: np.ndarray, epoch: int = 500):\n",
    "        with tqdm(total=epoch) as t:\n",
    "            for e in range(epoch):\n",
    "                pred = np.vectorize(Regressor.sigmoid)(np.dot(self.weights, X.T) + self.bias)\n",
    "\n",
    "                loss = -(Y * np.log(pred) + (1 - Y) * np.log(1 - pred))\n",
    "\n",
    "                dz = pred - Y\n",
    "                self.weights -= np.mean(np.multiply(dz, X.T), axis=1) * self.lr\n",
    "                self.bias -= np.mean(dz) * self.lr\n",
    "\n",
    "                t.set_postfix(loss=loss.mean())\n",
    "                t.update()\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.vectorize(Regressor.sigmoid)(np.dot(self.weights, X.T) + self.bias)\n",
    "    \n",
    "    def test(self, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        return np.mean(np.rint(self.predict(X)) == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('score:', network.test(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Regressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.vectorize(Regressor.sigmoid)(np.dot(network.weights, X.T) + network.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-(Y * np.log(pred) + (1 - Y) * np.log(1 - pred))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75093799, 0.79072221, 0.79027247, ..., 0.7927926 , 0.83392317,\n",
       "       0.82973434])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.vectorize(Regressor.sigmoid)(np.dot(self.weights, X.T) + self.bias)\n",
    "\n",
    "loss = \n",
    "\n",
    "dz = pred - Y\n",
    "self.weights -= np.mean(np.multiply(dz, X.T), axis=1) * self.lr\n",
    "self.bias -= np.mean(dz) * self.lr\n",
    "\n",
    "t.set_postfix(loss=loss.mean())\n",
    "t.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head.X = train_x.T\n",
    "head.A = sigmoid(np.dot(head.params, train_x.T) + head.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-(Y * np.log(head.A) + (1 - Y) * np.log(1 - head.A))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        dZ = process(self.after.backward(grad)) if self.after else (self.A - grad)\n",
    "        \n",
    "        self.dW += np.dot(dZ, self.X.T) / np.size(self.X, 0)\n",
    "        self.db += np.sum(dZ, axis=1, keepdims=True) / np.size(self.X, 0)\n",
    "        \n",
    "        # Release previous input\n",
    "        self.X = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Layer(2, 4).append(\n",
    "    Layer(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = Layer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(BaseLayer):\n",
    "    def __init__(self, head: int, size: int = 1):\n",
    "        self.head = head\n",
    "        self.params = np.random.rand(size, head)\n",
    "        self.bias = np.random.rand(size, 1)\n",
    "        \n",
    "        self.after, self.before = None, None\n",
    "        self.X, self.A = None, None\n",
    "        self.dW, self.db = 0, 0\n",
    "    \n",
    "    def forward(self, X: np.ndarray, grad: bool = True):\n",
    "        if grad:\n",
    "            if self.X is not None:\n",
    "                warnings.warn(\"Layer backward is not called after forward.\")\n",
    "            self.X = X\n",
    "            \n",
    "        self.A = sigmoid(np.dot(self.params, X) + self.bias)\n",
    "        \n",
    "        return self.after.forward(self.A) if self.after else self.A\n",
    "    \n",
    "    def backward(self, grad: np.ndarray):\n",
    "        def process(g):\n",
    "            return np.multiply(np.dot(self.after.params.T, g), 1 - np.power(self.A, 2))\n",
    "        \n",
    "        if self.X is None:\n",
    "            raise Exception(\"Layer backward call must after forward.\")\n",
    "            \n",
    "        dZ = process(self.after.backward(grad)) if self.after else (self.A - grad)\n",
    "        \n",
    "        self.dW += np.dot(dZ, self.X.T) / np.size(self.X.T, 0)\n",
    "        self.db += np.sum(dZ, axis=1, keepdims=True) / np.size(self.X.T, 0)\n",
    "        \n",
    "        # Release previous input\n",
    "        self.X = None\n",
    "\n",
    "        return dZ\n",
    "    \n",
    "    def update(self, lr: float):\n",
    "        self.params = self.params - lr * self.dW\n",
    "        self.bias = self.bias - lr * self.db\n",
    "        \n",
    "        self.dW, self.db = 0, 0\n",
    "        \n",
    "        if self.after:\n",
    "            self.after.update(lr)\n",
    "        \n",
    "    def append(self, layer):\n",
    "        self.after = layer\n",
    "        layer.before = self\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
